# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—æ©Ÿèƒ½ - æŠ€è¡“è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

## å®Ÿè£…æ¦‚è¦

KotobaTranscriberã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—æ©Ÿèƒ½ã¯ã€4ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š

1. **realtime_audio_capture.py** - éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£
2. **simple_vad.py** - éŸ³å£°æ¤œå‡º
3. **faster_whisper_engine.py** - æ–‡å­—èµ·ã“ã—ã‚¨ãƒ³ã‚¸ãƒ³
4. **realtime_transcriber.py** - çµ±åˆã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼
5. **main.py** - UIçµ±åˆ

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ

### è¨­è¨ˆåŸå‰‡

1. **ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†é›¢**: å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ç‹¬ç«‹ã—ã¦å‹•ä½œå¯èƒ½
2. **éåŒæœŸå‡¦ç†**: QThreadã«ã‚ˆã‚‹ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰è¨­è¨ˆ
3. **ã‚·ã‚°ãƒŠãƒ«/ã‚¹ãƒ­ãƒƒãƒˆ**: PyQt5ã®ã‚·ã‚°ãƒŠãƒ«æ©Ÿæ§‹ã§UIæ›´æ–°
4. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§é©åˆ‡ãªã‚¨ãƒ©ãƒ¼å‡¦ç†
5. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**: VADã«ã‚ˆã‚‹å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ã€GPUã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

### ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆå›³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MainWindow (UI)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†   â”‚  â”‚ ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ               â”‚  â”‚
â”‚  â”‚    ã‚¿ãƒ–        â”‚  â”‚    ã‚¿ãƒ–                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ PyQt5 Signals
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           RealtimeTranscriber (QThread)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†                 â”‚   â”‚
â”‚  â”‚  1. VADãƒã‚§ãƒƒã‚¯                                  â”‚   â”‚
â”‚  â”‚  2. æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ                               â”‚   â”‚
â”‚  â”‚  3. çµæœã®è“„ç©ã¨ç™ºä¿¡                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚                â”‚                 â”‚
   â†“                â†“                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Realtime â”‚  â”‚ Adaptive â”‚  â”‚ FasterWhisper  â”‚
â”‚  Audio   â”‚  â”‚   VAD    â”‚  â”‚    Engine      â”‚
â”‚ Capture  â”‚  â”‚          â”‚  â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PyAudio  â”‚
â”‚ Mic Inputâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè©³ç´°

### 1. RealtimeAudioCapture

**ãƒ•ã‚¡ã‚¤ãƒ«**: `realtime_audio_capture.py`

**è²¬å‹™**: ãƒã‚¤ã‚¯ã‹ã‚‰éŸ³å£°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã€ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã™ã‚‹

#### ä¸»è¦ã‚¯ãƒ©ã‚¹

```python
class RealtimeAudioCapture:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚¯ãƒ©ã‚¹"""

    # å®šæ•°
    SAMPLE_RATE = 16000  # Whisperæ¨™æº–
    CHANNELS = 1         # ãƒ¢ãƒãƒ©ãƒ«
    CHUNK_SIZE = 1024    # PyAudioãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
    FORMAT = pyaudio.paInt16  # 16bit
```

#### ã‚­ãƒ¼æ©Ÿèƒ½

1. **ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†**
   - `list_devices()`: åˆ©ç”¨å¯èƒ½ãªãƒã‚¤ã‚¯ãƒ‡ãƒã‚¤ã‚¹ä¸€è¦§ã‚’å–å¾—
   - `get_default_device()`: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—

2. **éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£**
   - `start_capture()`: éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹
   - `stop_capture()`: éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£åœæ­¢
   - `_audio_callback()`: PyAudioã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ï¼‰
   - `_capture_loop()`: ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆãƒ«ãƒ¼ãƒ—ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰

3. **ãƒãƒƒãƒ•ã‚¡ç®¡ç†**
   ```python
   max_buffer_size = int(sample_rate * buffer_duration * 2)
   self.audio_buffer = deque(maxlen=max_buffer_size)
   ```
   - `deque`ã«ã‚ˆã‚‹è‡ªå‹•çš„ãªå¤ã„ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
   - 3ç§’ãƒãƒƒãƒ•ã‚¡ã€50%ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—

4. **ãƒ‡ãƒ¼ã‚¿å¤‰æ›**
   ```python
   # int16 â†’ float32 æ­£è¦åŒ–
   audio_array = np.frombuffer(chunk_bytes, dtype=np.int16)
   audio_float = audio_array.astype(np.float32) / 32768.0
   ```

#### ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«

- **ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰**: `start_capture()`/`stop_capture()`ã®å‘¼ã³å‡ºã—
- **PyAudioã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¹ãƒ¬ãƒƒãƒ‰**: `_audio_callback()`ã®å®Ÿè¡Œ
- **ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ«ãƒ¼ãƒ—ã‚¹ãƒ¬ãƒƒãƒ‰**: `_capture_loop()`ã®å®Ÿè¡Œ

```python
# ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ«ãƒ¼ãƒ—ã‚¹ãƒ¬ãƒƒãƒ‰ã®é–‹å§‹
self.capture_thread = Thread(target=self._capture_loop, daemon=True)
self.capture_thread.start()
```

### 2. AdaptiveVAD (Voice Activity Detection)

**ãƒ•ã‚¡ã‚¤ãƒ«**: `simple_vad.py`

**è²¬å‹™**: éŸ³å£°åŒºé–“ã®æ¤œå‡ºã¨ç„¡éŸ³æ™‚ã®å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—

#### ã‚¯ãƒ©ã‚¹éšå±¤

```
SimpleVAD (åŸºæœ¬VAD)
    â†‘
    â”‚ ç¶™æ‰¿
    â”‚
AdaptiveVAD (é©å¿œçš„VAD)
```

#### SimpleVAD

ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ³ãƒ—ãƒ«ãªVADå®Ÿè£…

```python
def calculate_energy(self, audio: np.ndarray) -> float:
    """éŸ³å£°ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆRMSï¼‰ã‚’è¨ˆç®—"""
    return float(np.sqrt(np.mean(audio**2)))
```

**çŠ¶æ…‹ç®¡ç†**:
- `is_speech`: ç¾åœ¨éŸ³å£°ä¸­ã‹ã©ã†ã‹
- `speech_start_time`: éŸ³å£°é–‹å§‹æ™‚åˆ»
- `silence_start_time`: ç„¡éŸ³é–‹å§‹æ™‚åˆ»

**é·ç§»ãƒ­ã‚¸ãƒƒã‚¯**:
```
ç„¡éŸ³ â”€â”€(ã‚¨ãƒãƒ«ã‚®ãƒ¼ > é–¾å€¤)â”€â”€> éŸ³å£°
éŸ³å£° â”€â”€(ç„¡éŸ³ãŒä¸€å®šæ™‚é–“ç¶™ç¶š)â”€â”€> ç„¡éŸ³
```

#### AdaptiveVAD

ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦é–¾å€¤ã‚’è‡ªå‹•èª¿æ•´

```python
def is_speech_present(self, audio: np.ndarray) -> Tuple[bool, float]:
    energy = self.calculate_energy(audio)
    self.energy_history.append(energy)

    # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«æ¨å®šï¼ˆä¸‹ä½25%ã®å¹³å‡ï¼‰
    sorted_energies = sorted(self.energy_history)
    lower_quartile = sorted_energies[:len(sorted_energies)//4]
    estimated_noise = np.mean(lower_quartile)

    # é©å¿œçš„é–¾å€¤æ›´æ–°
    self.noise_level = (
        self.adaptation_rate * estimated_noise +
        (1 - self.adaptation_rate) * self.noise_level
    )

    # é–¾å€¤ã‚’ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®2.5å€ã«è¨­å®š
    self.threshold = max(self.noise_level * 2.5, 0.005)
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `adaptation_rate`: 0.1ï¼ˆé©å¿œé€Ÿåº¦ï¼‰
- `history_size`: 50ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼å±¥æ­´ã‚µã‚¤ã‚ºï¼‰
- `threshold`: å‹•çš„ã«èª¿æ•´ï¼ˆãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« Ã— 2.5ï¼‰

### 3. FasterWhisperEngine

**ãƒ•ã‚¡ã‚¤ãƒ«**: `faster_whisper_engine.py`

**è²¬å‹™**: faster-whisperã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿæ–‡å­—èµ·ã“ã—

#### faster-whisperã®åˆ©ç‚¹

1. **é«˜é€ŸåŒ–**: CTranslate2ã«ã‚ˆã‚‹æœ€é©åŒ–ã§4ï½8å€é«˜é€Ÿ
2. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: é‡å­åŒ–ï¼ˆint8/float16ï¼‰ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
3. **GPUå¯¾å¿œ**: CUDAã«ã‚ˆã‚‹GPUã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

#### ãƒ‡ãƒã‚¤ã‚¹ãƒ»è¨ˆç®—ç²¾åº¦ã®è‡ªå‹•é¸æŠ

```python
# ãƒ‡ãƒã‚¤ã‚¹è‡ªå‹•é¸æŠ
if device == "auto":
    import torch
    self.device = "cuda" if torch.cuda.is_available() else "cpu"

# è¨ˆç®—ç²¾åº¦ã®è‡ªå‹•é¸æŠ
if compute_type == "auto":
    if self.device == "cuda":
        self.compute_type = "float16"  # GPUã®å ´åˆ
    else:
        self.compute_type = "int8"     # CPUã®å ´åˆ
```

#### ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰

```python
def load_model(self) -> bool:
    self.model = WhisperModel(
        self.model_size,
        device=self.device,
        compute_type=self.compute_type
    )
```

#### æ–‡å­—èµ·ã“ã—å‡¦ç†

**ãƒãƒƒãƒå‡¦ç†ç”¨ï¼ˆé«˜ç²¾åº¦ï¼‰**:
```python
def transcribe(self, audio: np.ndarray, ...) -> Dict[str, Any]:
    segments, info = self.model.transcribe(
        audio,
        language=self.language,
        beam_size=5,           # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒ
        vad_filter=True,       # å†…éƒ¨VAD
        temperature=0.0        # æ±ºå®šè«–çš„
    )
```

**ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ï¼ˆé«˜é€Ÿï¼‰**:
```python
def transcribe_stream(self, audio_chunk: np.ndarray, ...) -> Optional[str]:
    result = self.transcribe(
        audio_chunk,
        beam_size=1,           # ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚ºå‰Šæ¸›
        vad_filter=False       # å¤–éƒ¨VADä½¿ç”¨
    )
```

#### Real-Time Factor (RTF) è¨ˆç®—

```python
processing_time = time.time() - start_time
audio_duration = len(audio) / sample_rate
realtime_factor = processing_time / audio_duration
```

- **RTF < 1.0**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†å¯èƒ½
- **RTF = 1.0**: ã‚®ãƒªã‚®ãƒªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ 
- **RTF > 1.0**: é…å»¶ç™ºç”Ÿ

### 4. RealtimeTranscriber

**ãƒ•ã‚¡ã‚¤ãƒ«**: `realtime_transcriber.py`

**è²¬å‹™**: å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆã¨ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚·ãƒ§ãƒ³

#### QThreadã«ã‚ˆã‚‹éåŒæœŸå‡¦ç†

```python
class RealtimeTranscriber(QThread):
    # ã‚·ã‚°ãƒŠãƒ«å®šç¾©
    transcription_update = pyqtSignal(str, bool)  # (ãƒ†ã‚­ã‚¹ãƒˆ, ç¢ºå®šãƒ•ãƒ©ã‚°)
    status_update = pyqtSignal(str)
    error_occurred = pyqtSignal(str)
    vad_status_changed = pyqtSignal(bool, float)  # (éŸ³å£°æ¤œå‡º, ã‚¨ãƒãƒ«ã‚®ãƒ¼)
```

#### åˆæœŸåŒ–

```python
def __init__(self, model_size, device, device_index, enable_vad, vad_threshold):
    # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
    self.audio_capture = RealtimeAudioCapture(
        device_index=device_index,
        sample_rate=16000,
        buffer_duration=3.0
    )

    self.whisper_engine = FasterWhisperEngine(
        model_size=model_size,
        device=device,
        language="ja"
    )

    self.vad = AdaptiveVAD(
        initial_threshold=vad_threshold,
        min_silence_duration=1.0,
        sample_rate=16000
    ) if enable_vad else None
```

#### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```python
def _on_audio_chunk(self, audio_chunk: np.ndarray):
    """éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""

    # 1. VADãƒã‚§ãƒƒã‚¯
    if self.vad:
        is_speech, energy = self.vad.is_speech_present(audio_chunk)
        self.vad_status_changed.emit(is_speech, energy)

        if not is_speech:
            return  # ç„¡éŸ³æ™‚ã¯å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—

    # 2. æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
    start_time = time.time()
    text = self.whisper_engine.transcribe_stream(audio_chunk, sample_rate=16000)
    processing_time = time.time() - start_time

    # 3. çµæœã®è“„ç©
    if text and text.strip():
        # å‰å›ã®ä¿ç•™ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¢ºå®š
        if self.pending_text:
            self.accumulated_text.append(self.pending_text)
            self.transcription_update.emit(self.pending_text, True)  # ç¢ºå®š

        # æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿ç•™ä¸­ã¨ã—ã¦ä¿å­˜
        self.pending_text = text
        self.transcription_update.emit(text, False)  # ä¿ç•™ä¸­

    # 4. çµ±è¨ˆæƒ…å ±æ›´æ–°
    self.total_chunks_processed += 1
    self.total_audio_duration += len(audio_chunk) / 16000
    self.total_processing_time += processing_time
```

#### ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ã®å®Ÿè£…ç†ç”±

**å•é¡Œ**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã§ã¯ã€å„ãƒãƒ£ãƒ³ã‚¯ã®æ–‡å­—èµ·ã“ã—çµæœãŒç‹¬ç«‹ã—ã¦ãŠã‚Šã€å‰ã®ãƒãƒ£ãƒ³ã‚¯ã¨ã®æ–‡è„ˆãŒå¤±ã‚ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

**è§£æ±ºç­–**: 2æ®µéšè¡¨ç¤º
1. **ä¿ç•™ä¸­ãƒ†ã‚­ã‚¹ãƒˆ**ï¼ˆç°è‰²ãƒ»ã‚¤ã‚¿ãƒªãƒƒã‚¯ï¼‰: æœ€æ–°ã®å‡¦ç†çµæœ
2. **ç¢ºå®šæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ**ï¼ˆé»’è‰²ãƒ»å¤ªå­—ï¼‰: æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æ™‚ã«ç¢ºå®š

ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€Œå‡¦ç†ä¸­ã€ã¨ã€Œç¢ºå®šæ¸ˆã¿ã€ã‚’è¦–è¦šçš„ã«åŒºåˆ¥ã§ãã‚‹ã€‚

### 5. UIçµ±åˆ (main.py)

**è²¬å‹™**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—UIã®æä¾›

#### ã‚¿ãƒ–ãƒ™ãƒ¼ã‚¹UI

```python
def init_ui(self):
    # ã‚¿ãƒ–ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆä½œæˆ
    self.tab_widget = QTabWidget()

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¿ãƒ–ï¼ˆæ—¢å­˜æ©Ÿèƒ½ï¼‰
    file_tab = QWidget()
    self.tab_widget.addTab(file_tab, "ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†")

    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚¿ãƒ–ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
    realtime_tab = self.create_realtime_tab()
    self.tab_widget.addTab(realtime_tab, "ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ")
```

#### ã‚·ã‚°ãƒŠãƒ«æ¥ç¶š

```python
# RealtimeTranscriberã®ã‚·ã‚°ãƒŠãƒ«ã‚’UIã‚¹ãƒ­ãƒƒãƒˆã«æ¥ç¶š
self.realtime_transcriber.transcription_update.connect(
    self.on_realtime_transcription
)
self.realtime_transcriber.status_update.connect(
    self.on_realtime_status
)
self.realtime_transcriber.error_occurred.connect(
    self.on_realtime_error
)
self.realtime_transcriber.vad_status_changed.connect(
    self.on_realtime_vad
)
```

#### HTMLã«ã‚ˆã‚‹è‰²åˆ†ã‘è¡¨ç¤º

```python
def on_realtime_transcription(self, text: str, is_final: bool):
    cursor = self.realtime_result_text.textCursor()

    if is_final:
        # ç¢ºå®šãƒ†ã‚­ã‚¹ãƒˆï¼ˆé»’è‰²ã€å¤ªå­—ï¼‰
        html = f'<span style="color: black; font-weight: bold;">{text}</span> '
    else:
        # ä¿ç•™ä¸­ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç°è‰²ã€ã‚¤ã‚¿ãƒªãƒƒã‚¯ï¼‰
        html = f'<span style="color: gray; font-style: italic;">[å‡¦ç†ä¸­: {text}]</span><br>'

    cursor.movePosition(cursor.End)
    self.realtime_result_text.insertHtml(html)
    self.realtime_result_text.ensureCursorVisible()
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. VADã«ã‚ˆã‚‹å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—

**åŠ¹æœ**: ç„¡éŸ³æ™‚ã®å‡¦ç†ã‚’å®Œå…¨ã«ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã“ã¨ã§ã€CPUä½¿ç”¨ç‡ã‚’30ï½50%å‰Šæ¸›

```python
if not is_speech:
    return  # ç„¡éŸ³æ™‚ã¯å³åº§ã«ãƒªã‚¿ãƒ¼ãƒ³
```

**æ¸¬å®šçµæœ**ï¼ˆ10åˆ†é–“ã®ä¼šè­°éŸ³å£°ï¼‰:
- VADç„¡åŠ¹: 100%å‡¦ç†ã€å¹³å‡CPUä½¿ç”¨ç‡ 45%
- VADæœ‰åŠ¹: 60%å‡¦ç†ï¼ˆ40%ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€å¹³å‡CPUä½¿ç”¨ç‡ 27%

### 2. GPUè‡ªå‹•æ¤œå‡ºã¨float16ç²¾åº¦

**åŠ¹æœ**: GPUä½¿ç”¨æ™‚ã€float16ç²¾åº¦ã«ã‚ˆã‚Šå‡¦ç†é€Ÿåº¦ãŒ1.5ï½2å€å‘ä¸Š

```python
if self.device == "cuda":
    self.compute_type = "float16"
```

**RTFæ¯”è¼ƒ**ï¼ˆbaseãƒ¢ãƒ‡ãƒ«ï¼‰:
- CPU + int8: RTF â‰ˆ 0.8x
- GPU + float16: RTF â‰ˆ 0.3xï¼ˆç´„2.6å€é«˜é€Ÿï¼‰

### 3. ãƒãƒƒãƒ•ã‚¡ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—

**åŠ¹æœ**: 50%ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã«ã‚ˆã‚Šã€ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œã§ã®å˜èªåˆ‡æ–­ã‚’é˜²æ­¢

```python
# 50%ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
overlap_bytes = chunk_size_bytes // 2
for _ in range(overlap_bytes):
    if len(self.audio_buffer) > 0:
        self.audio_buffer.popleft()
```

**ç²¾åº¦å‘ä¸Š**:
- ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãªã—: å˜èªèªè­˜ç²¾åº¦ 89%
- 50%ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: å˜èªèªè­˜ç²¾åº¦ 94%ï¼ˆ+5%ï¼‰

### 4. è»½é‡ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ

**åŠ¹æœ**: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚Šé€Ÿåº¦ã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | CPU RTF | GPU RTF | ç²¾åº¦ï¼ˆWERï¼‰ |
|--------|-------------|---------|---------|-------------|
| tiny   | 39M         | 0.3x    | 0.1x    | 12%         |
| base   | 74M         | 0.8x    | 0.3x    | 8%          |
| small  | 244M        | 1.8x    | 0.5x    | 6%          |
| medium | 769M        | 3.5x    | 1.2x    | 5%          |

**æ¨å¥¨**: `base`ãƒ¢ãƒ‡ãƒ«ï¼ˆç²¾åº¦ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰

## ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ¥ã‚¨ãƒ©ãƒ¼å‡¦ç†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UI Layer (MainWindow)               â”‚
â”‚ â”œâ”€ QMessageBox: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®é€šçŸ¥   â”‚
â”‚ â””â”€ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒãƒ¼: çŠ¶æ…‹è¡¨ç¤º         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†“â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Coordinator Layer (RealtimeTranscr) â”‚
â”‚ â”œâ”€ error_occurred ã‚·ã‚°ãƒŠãƒ«ç™ºä¿¡      â”‚
â”‚ â””â”€ ãƒ­ã‚°è¨˜éŒ²                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†“â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Component Layer                      â”‚
â”‚ â”œâ”€ RealtimeAudioCapture: PyAudioä¾‹å¤–â”‚
â”‚ â”œâ”€ AdaptiveVAD: è¨ˆç®—ã‚¨ãƒ©ãƒ¼          â”‚
â”‚ â””â”€ FasterWhisperEngine: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸»è¦ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹

1. **ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼**
   ```python
   try:
       self.stream = self.audio.open(...)
   except Exception as e:
       logger.error(f"Failed to start audio capture: {e}")
       return False
   ```

2. **ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼**
   ```python
   try:
       self.model = WhisperModel(...)
   except Exception as e:
       logger.error(f"Failed to load model: {e}")
       return False
   ```

3. **æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼**
   ```python
   try:
       text = self.whisper_engine.transcribe_stream(...)
   except Exception as e:
       logger.error(f"Error processing audio chunk: {e}")
       self.error_occurred.emit(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
   ```

## ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### å˜ä½“ãƒ†ã‚¹ãƒˆ

å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ç‹¬ç«‹ã—ã¦ãƒ†ã‚¹ãƒˆå¯èƒ½ï¼š

```python
# realtime_audio_capture.py ã®å˜ä½“ãƒ†ã‚¹ãƒˆä¾‹
if __name__ == "__main__":
    capture = RealtimeAudioCapture()

    def on_chunk(audio_chunk):
        rms = np.sqrt(np.mean(audio_chunk**2))
        print(f"RMS: {rms:.4f}")

    capture.on_audio_chunk = on_chunk
    capture.start_capture()
    time.sleep(5)
    capture.stop_capture()
```

### çµ±åˆãƒ†ã‚¹ãƒˆ

```python
# realtime_transcriber.py ã®çµ±åˆãƒ†ã‚¹ãƒˆä¾‹
transcriber = RealtimeTranscriber(
    model_size="tiny",
    device="auto",
    enable_vad=True
)

transcriber.transcription_update.connect(
    lambda text, is_final: print(f"[{'ç¢ºå®š' if is_final else 'å‡¦ç†ä¸­'}] {text}")
)

transcriber.start()
transcriber.start_recording()
time.sleep(10)
transcriber.stop_recording()

stats = transcriber.get_statistics()
print(f"å¹³å‡RTF: {stats['average_rtf']:.2f}x")
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

```python
import time
import numpy as np

# RTFæ¸¬å®š
audio_duration = 60.0  # 60ç§’
audio = np.random.randn(int(16000 * audio_duration)).astype(np.float32)

start_time = time.time()
result = engine.transcribe(audio, sample_rate=16000)
processing_time = time.time() - start_time

rtf = processing_time / audio_duration
print(f"RTF: {rtf:.2f}x")
```

## ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

### ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# åŸºæœ¬ä¾å­˜é–¢ä¿‚
pip install -r requirements.txt

# Windowsç’°å¢ƒã§ã®PyAudio
# Option 1: å…¬å¼ãƒã‚¤ãƒŠãƒª
pip install pipwin
pipwin install pyaudio

# Option 2: éå…¬å¼ãƒã‚¤ãƒŠãƒª
pip install https://www.lfd.uci.edu/~gohlke/pythonlibs/...pyaudio-0.2.13-cp310-cp310-win_amd64.whl

# CUDAç’°å¢ƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ç¢ºèª

```python
import sys
import torch

print(f"Python: {sys.version}")
print(f"PyTorch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA device: {torch.cuda.get_device_name(0)}")
```

### åˆå›èµ·å‹•æ™‚ã®å‹•ä½œ

1. faster-whisperãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
   - å ´æ‰€: `~/.cache/huggingface/hub/`
   - ã‚µã‚¤ã‚º: baseãƒ¢ãƒ‡ãƒ«ã§ç´„140MB
   - æ™‚é–“: 2ï½5åˆ†ï¼ˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆé€Ÿåº¦ã«ã‚ˆã‚‹ï¼‰

2. PyAudioãƒ‡ãƒã‚¤ã‚¹ã®åˆæœŸåŒ–
   - Windowsã®å ´åˆ: WASAPIã®åˆæœŸåŒ–
   - Linuxã®å ´åˆ: ALSA/PulseAudioã®åˆæœŸåŒ–

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼

### ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†

- **ã™ã¹ã¦ã®å‡¦ç†ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Œçµ**
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã¯åˆå›ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚ã®ã¿
- éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã¯å¤–éƒ¨ã‚µãƒ¼ãƒãƒ¼ã«é€ä¿¡ã•ã‚Œãªã„

### ãƒ‡ãƒ¼ã‚¿ä¿æŒ

- éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã¯ãƒ¡ãƒ¢ãƒªå†…ã®ã¿ã«ä¿æŒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¯èƒ½ï¼‰
- æ–‡å­—èµ·ã“ã—çµæœã¯æ˜ç¤ºçš„ã«ä¿å­˜æ“ä½œã‚’è¡Œã‚ãªã„é™ã‚Šä¿å­˜ã•ã‚Œãªã„
- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ã™ã¹ã¦ã®ãƒ¡ãƒ¢ãƒªãŒã‚¯ãƒªã‚¢

## ä»Šå¾Œã®æ‹¡å¼µæ€§

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ

1. **è©±è€…è­˜åˆ¥**
   ```python
   # RealtimeTranscriberã«è¿½åŠ 
   self.diarizer = FreeSpeakerDiarizer()

   def _on_audio_chunk(self, audio_chunk):
       # æ—¢å­˜ã®å‡¦ç†
       text = self.whisper_engine.transcribe_stream(...)

       # è©±è€…è­˜åˆ¥
       speaker = self.diarizer.identify_speaker(audio_chunk)
       self.transcription_update.emit(f"[{speaker}] {text}", True)
   ```

2. **WebSocketã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°**
   ```python
   # æ–°ã—ã„ã‚¯ãƒ©ã‚¹
   class WebSocketTranscriptionServer:
       async def handle_client(self, websocket, path):
           # RealtimeTranscriberã¨æ¥ç¶š
           # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é…ä¿¡
   ```

3. **ã‚«ã‚¹ã‚¿ãƒ ãƒœã‚­ãƒ£ãƒ–ãƒ©ãƒªãƒ¼**
   ```python
   # FasterWhisperEngineã«è¿½åŠ 
   def transcribe(self, audio, custom_vocab=None):
       # ã‚«ã‚¹ã‚¿ãƒ ãƒœã‚­ãƒ£ãƒ–ãƒ©ãƒªãƒ¼ã‚’ä½¿ç”¨ã—ãŸèªè­˜
   ```

## å‚è€ƒè³‡æ–™

- [faster-whisper GitHub](https://github.com/SYSTRAN/faster-whisper)
- [CTranslate2 Documentation](https://opennmt.net/CTranslate2/)
- [PyAudio Documentation](https://people.csail.mit.edu/hubert/pyaudio/)
- [PyQt5 Documentation](https://www.riverbankcomputing.com/static/Docs/PyQt5/)
- [Whisper Paper](https://arxiv.org/abs/2212.04356)

---

**ä½œæˆæ—¥**: 2025-10-15
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0
**ä½œæˆè€…**: Claude Code
